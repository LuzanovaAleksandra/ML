---
title: "Natural Language Processing"
output:
  word_document: default
  html_notebook: default
  pdf_document: default
  html_document: default
---

# Importing the dataset

```{r}
#Параметр quote = '' дозволяє позбутися лапок в тексті вже на етапі читання файлу

#Параметр stringsAsFactors = FALSE дозволяє не розпізнавати зміст відгуків як значення факторних змінних

dataset = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
```

# Cleaning the texts

```{r}
#install.packages('tm')
#install.packages('SnowballC')
library(tm)
library(SnowballC)

#Для початку роботи з відгуками слід створити корпус текстів
corpus = VCorpus(VectorSource(dataset$Review))

#Для роботи з корпусом використовуємо функцію tm_map
#Переводимо всі букви в нижній регістр щоб уникнути повторів слів в різному регістрі
corpus = tm_map(corpus, content_transformer(tolower))

#Видаляємо числа з текстів
corpus = tm_map(corpus, removeNumbers)

#Видаляємо пунктуацію
corpus = tm_map(corpus, removePunctuation)

#Видаляємо "стоп-слова", таких як this, why, etc., використовуємо для цього функцію "stopwords"; ця функція з бібліотеки SnowballC
corpus = tm_map(corpus, removeWords, stopwords())

#Проводимо стеммірування, зберігаючи тільки корінь слова
corpus = tm_map(corpus, stemDocument)

#Видаляємо зайві пробіли, які утворилися при видаленні стоп-слів та інших перетворень
corpus = tm_map(corpus, stripWhitespace)
```

# Creating the Bag of Words model
```{r}
#Для створення моделі "мішок слів" в якості незалежної змінної виступає створений вище корпус текстів. Конвертуємо його в матрицю
dtm = DocumentTermMatrix(corpus)

#Зверненням до матриці - dtm - можна подивитися її характеристики. В даному випадку, ми отримали матрицю розмірністю 1000*1577, з 100% розрідженістю. Для скорочення її розмірності і розрідженості, видалимо найменш частотні слова. Залишимо 99,9% найбільш частотних слів. В результаті, розмірність матриці скоротилася до 1000*691, розрідженість - 99%.
dtm = removeSparseTerms(dtm, 0.999)

#Для побудови класифікатора, отриману матрицю необхідно перетворити в дата-фрейм - as.data.frame. При цьому, потрібно уточнити, що ми звертаємося до неї, як до матриці - as.matrix
ds = as.data.frame(as.matrix(dtm))

#Додамо стовпець для ендогенної змінної, в якій зафіксований характер відгуку - позитивний чи негативний
ds$Liked = dataset$Liked

#Оголосимо тип залежної змінної - якісна (factor)
ds$Liked = factor(ds$Liked, levels = c(0, 1))
```

# Splitting the dataset into the Training set and Test set
```{r}
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(ds$Liked, SplitRatio = 0.8)
training_set = subset(ds, split == TRUE)
test_set = subset(ds, split == FALSE)
```

# Fitting Random Forest Classification to the Training set
```{r}
# install.packages('randomForest')
library(randomForest)
classifier = randomForest(x = training_set[-692],
                          y = training_set$Liked,
                          ntree = 10)
```

## Predicting the Test set results
```{r}
y_pred = predict(classifier, newdata = test_set[-692])
```

## Making the Confusion Matrix
```{r}
cm = table(test_set[, 692], y_pred)
cm
```